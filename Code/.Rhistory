# Streamlined Modeling Script - STARTS AFTER SPLITTING
# This script loads pre-existing train/test splits and runs the full analysis
library(here)
library(dplyr)
library(tidymodels)
library(ranger)
library(kernlab)
library(kknn)
library(ggplot2)
library(viridis)
library(scales)
library(forcats)
library(tidyr)
NATAL_ISO_THRESHOLD <- 0.713
set.seed(123)
data_types <- c("RAW", "GAM", "MA")
# Input directories (where train/test splits already exist)
train_test_dir_total <- here("Data", "TrainingTesting")
train_test_dir_overlap <- here("Data", "TrainingTesting", "Filtered")
# Output directories
models_dir_total <- here("Output", "Models", "Total")
models_dir_overlap <- here("Output", "Models", "Filtered")
results_dir_total <- here("Output", "ModelResultsPreCal", "Total")
results_dir_overlap <- here("Output", "ModelResultsPreCal", "Filtered")
figures_dir <- here("Figures", "ModelPerformance")
for(dir in c(models_dir_total, models_dir_overlap, results_dir_total, results_dir_overlap, figures_dir)) {
dir.create(dir, recursive = TRUE, showWarnings = FALSE)
}
cat("\n=== Checking for required train/test split files ===\n")
# Check Total analysis files
total_files_exist <- TRUE
for (data_type in data_types) {
train_file <- here(train_test_dir_total, paste0("Train_", data_type, ".csv"))
test_file <- here(train_test_dir_total, paste0("Test_", data_type, ".csv"))
if (!file.exists(train_file)) {
cat("✗ MISSING:", train_file, "\n")
total_files_exist <- FALSE
} else {
cat("✓ Found:", basename(train_file), "\n")
}
if (!file.exists(test_file)) {
cat("✗ MISSING:", test_file, "\n")
total_files_exist <- FALSE
} else {
cat("✓ Found:", basename(test_file), "\n")
}
}
# Check Overlap/Filtered analysis files
overlap_files_exist <- TRUE
for (data_type in data_types) {
train_file <- here(train_test_dir_overlap, paste0("Train_", data_type, ".csv"))
test_file <- here(train_test_dir_overlap, paste0("Test_", data_type, ".csv"))
if (!file.exists(train_file)) {
cat("✗ MISSING:", train_file, "\n")
overlap_files_exist <- FALSE
} else {
cat("✓ Found (Filtered):", basename(train_file), "\n")
}
if (!file.exists(test_file)) {
cat("✗ MISSING:", test_file, "\n")
overlap_files_exist <- FALSE
} else {
cat("✓ Found (Filtered):", basename(test_file), "\n")
}
}
# Check for Fish_ID_Splits.csv (optional but helpful)
fish_id_file <- here(train_test_dir_total, "Fish_ID_Splits.csv")
if (file.exists(fish_id_file)) {
cat("✓ Found: Fish_ID_Splits.csv\n")
fish_splits <- read.csv(fish_id_file)
cat("  Train Fish IDs:", sum(fish_splits$Split == "Train"), "\n")
cat("  Test Fish IDs:", sum(fish_splits$Split == "Test"), "\n")
} else {
cat("⚠ Fish_ID_Splits.csv not found (optional)\n")
}
if (!total_files_exist || !overlap_files_exist) {
stop("\n❌ ERROR: Missing required train/test split files. Please run the splitting script first.\n")
}
cat("\n✓ All required files found! Proceeding with analysis...\n")
run_analysis <- function(train_test_dir, models_dir, analysis_name, results_dir) {
results <- data.frame()
for (data_type in data_types) {
train_file <- here(train_test_dir, paste0("Train_", data_type, ".csv"))
test_file <- here(train_test_dir, paste0("Test_", data_type, ".csv"))
if (!file.exists(train_file) || !file.exists(test_file)) next
train_data <- read.csv(train_file) %>% mutate(Watershed = as.factor(Watershed))
test_data <- read.csv(test_file) %>% mutate(Watershed = as.factor(Watershed))
if (nrow(test_data) == 0) next
cat("Processing", data_type, "- Train:", nrow(train_data), "Test:", nrow(test_data), "\n")
base_recipe <- recipe(Watershed ~ ., data = train_data)
n_predictors <- ncol(train_data) - 1
models <- list(
RF = rand_forest(trees = 500, mtry = floor(sqrt(n_predictors))) %>% set_engine("ranger") %>% set_mode("classification"),
SVM = svm_rbf() %>% set_engine("kernlab") %>% set_mode("classification"),
KNN = nearest_neighbor(neighbors = 5) %>% set_engine("kknn") %>% set_mode("classification")
)
for (model_name in names(models)) {
set.seed(123)
workflow_obj <- workflow() %>% add_recipe(base_recipe) %>% add_model(models[[model_name]]) %>% fit(train_data)
saveRDS(workflow_obj, here(models_dir, paste0(data_type, "_", model_name, "_model.rds")))
predictions <- workflow_obj %>% predict(test_data) %>% bind_cols(test_data %>% select(Watershed))
pred_probs <- workflow_obj %>% predict(test_data, type = "prob")
predictions_with_probs <- predictions %>% bind_cols(pred_probs) %>% mutate(Dataset = data_type, Model = model_name, Correct = Watershed == .pred_class)
write.csv(predictions_with_probs, here(results_dir, paste0(data_type, "_", model_name, "_", analysis_name, "_predictions.csv")), row.names = FALSE)
accuracy <- mean(predictions$Watershed == predictions$.pred_class)
f1_score <- predictions %>% f_meas(truth = Watershed, estimate = .pred_class) %>% pull(.estimate)
results <- rbind(results, data.frame(Dataset = data_type, Model = model_name, Accuracy = round(accuracy, 3), F1_Score = round(f1_score, 3)))
}
}
results <- results[order(-results$Accuracy), ]
cat("\n=== Results for", analysis_name, "===\n")
print(results)
return(results)
}
cat("\n=== Running TOTAL analysis ===\n")
results_total <- run_analysis(train_test_dir_total, models_dir_total, "TOTAL", results_dir_total)
